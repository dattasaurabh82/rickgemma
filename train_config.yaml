# The Model: We use the Base version
model: "mlx-community/gemma-2-2b"

# Training Settings
train: true
data: "data"          # Folder containing our processed train.jsonl
batch_size: 4
iters: 600            # Increased iterations to ensure the style sinks in

# Stronger Brain Settings
lora_parameters:
  rank: 64            # High rank to capture complex stylistic nuances
  alpha: 128          # High alpha to make the updates forceful
  dropout: 0.0
  scale: 2.0

# Save settings
adapter_path: "adapters"
save_every: 100

